{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_PRACT11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQGQyLQUsRJ1peZzaF4fDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simran-sharma29/MSC-IT-PART-2/blob/main/NLP/PRACT11/NLP_PRACT11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. a) Multiword Expressions in NLP\n",
        "Source code:"
      ],
      "metadata": {
        "id": "Ao6LZPkVjPwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8azOYXKRjKRx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Al2rM0Iweu",
        "outputId": "06b221f7-68a6-4710-f852-318f019dbba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n",
        "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n",
        "for sent in sent_tokenize(s):\n",
        " print(mwe.tokenize(word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QMPqPXLIkNE",
        "outputId": "1a9e6e30-38f5-4795-f0ba-19ffe5df5686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Normalized Web Distance and Word Similarity"
      ],
      "metadata": {
        "id": "K4xEbceCjWyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n23rjpfpJbwT",
        "outputId": "0fb9dc7d-f909-4dfb-d51e-0445f72099c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import textdistance\n",
        "# we will need scikit-learn>=0.21\n",
        "import sklearn #pip install sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts = ['Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market','Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport', 'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n"
      ],
      "metadata": {
        "id": "93i6AgM2JRwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        " \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        " return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
        "def group_texts(texts, threshold=0.4): \n",
        " \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        " normalized_texts = np.array([normalize(text) for text in texts])\n",
        " distances = 1 - np.array([\n",
        " [textdistance.jaro_winkler(one, another) for one in normalized_texts] \n",
        " for another in normalized_texts\n",
        " ])\n",
        " clustering = AgglomerativeClustering(\n",
        " distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
        " affinity=\"precomputed\", linkage=\"complete\", n_clusters=None\n",
        " ).fit(distances)\n",
        " centers = dict()\n",
        " for cluster_id in set(clustering.labels_):\n",
        "  index = clustering.labels_ == cluster_id\n",
        "  centrality = distances[:, index][index].sum(axis=1)\n",
        "  centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        " return [centers[i] for i in clustering.labels_]\n",
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPrraOLsJjWX",
        "outputId": "18a6af20-9e77-4f27-b2d2-53bfc4f2f9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Word Sense Disambiguation "
      ],
      "metadata": {
        "id": "g02rVp_ljgKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Sense Disambiguation\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_first_sense(word, pos=None):\n",
        " if pos:\n",
        "  synsets = wn.synsets(word,pos)\n",
        " else:\n",
        "  synsets = wn.synsets(word)\n",
        " return synsets[0]\n",
        "best_synset = get_first_sense('bank')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','n')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "best_synset = get_first_sense('set','v')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YkhNcZ6KBDd",
        "outputId": "703e45da-870e-4c74-8136-8a72201e0686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        }
      ]
    }
  ]
}